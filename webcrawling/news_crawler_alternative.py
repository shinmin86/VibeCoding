#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ëŒ€ì•ˆì ì¸ ë‰´ìŠ¤ í¬ë¡¤ë§ ë°©ë²•ë“¤ì„ ì œê³µí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
ë„¤ì´ë²„ ì§ì ‘ í¬ë¡¤ë§ì´ ì–´ë ¤ìš¸ ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ ë°©ë²•ë“¤
"""

import requests
from bs4 import BeautifulSoup
import time
import csv
import json
from datetime import datetime
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Border, Side, Alignment
from openpyxl.utils import get_column_letter


class AlternativeNewsCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        # SSL ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™”
        self.session.verify = False
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    def crawl_daum_news(self, keyword="ë°˜ë„ì²´"):
        """
        ë‹¤ìŒ ë‰´ìŠ¤ì—ì„œ íŠ¹ì • í‚¤ì›Œë“œ ê´€ë ¨ ë‰´ìŠ¤ ì œëª©ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
        
        Args:
            keyword (str): ê²€ìƒ‰í•  í‚¤ì›Œë“œ
            
        Returns:
            list: ë‰´ìŠ¤ ì œëª© ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ë‹¤ìŒ ë‰´ìŠ¤ ê²€ìƒ‰ URL
            url = f"https://search.daum.net/search?w=news&q={keyword}&DA=PGD&spacing=0"
            print(f"ë‹¤ìŒ ë‰´ìŠ¤ ê²€ìƒ‰: {url}")
            
            response = self.session.get(url)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            news_titles = []
            
            # ë‹¤ìŒ ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ
            title_elements = soup.find_all('a', class_='f_link_b')
            
            for element in title_elements:
                title = element.get_text().strip()
                if title and len(title) > 5:
                    news_titles.append(title)
            
            print(f"ë‹¤ìŒì—ì„œ {len(news_titles)}ê°œì˜ ë‰´ìŠ¤ ì œëª©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return news_titles
            
        except Exception as e:
            print(f"ë‹¤ìŒ ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return []
    
    def crawl_google_news(self, keyword="ë°˜ë„ì²´ ê´€ë ¨ì£¼"):
        """
        êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ íŠ¹ì • í‚¤ì›Œë“œ ê´€ë ¨ ë‰´ìŠ¤ ì œëª©ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
        
        Args:
            keyword (str): ê²€ìƒ‰í•  í‚¤ì›Œë“œ
            
        Returns:
            list: ë‰´ìŠ¤ ì œëª© ë¦¬ìŠ¤íŠ¸
        """
        try:
            # êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ URL (í•œêµ­ ì§€ì—­)
            url = f"https://news.google.com/rss/search?q={keyword}&hl=ko&gl=KR&ceid=KR:ko"
            print(f"êµ¬ê¸€ ë‰´ìŠ¤ RSS ê²€ìƒ‰: {url}")
            
            response = self.session.get(url)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'xml')
            news_titles = []
            
            # RSS í”¼ë“œì—ì„œ ì œëª© ì¶”ì¶œ
            items = soup.find_all('item')
            
            for item in items:
                title_element = item.find('title')
                if title_element:
                    title = title_element.get_text().strip()
                    # êµ¬ê¸€ ë‰´ìŠ¤ì˜ ê²½ìš° "- ì¶œì²˜ëª…" í˜•íƒœë¡œ ëë‚˜ë¯€ë¡œ ì´ë¥¼ ì •ë¦¬
                    if ' - ' in title:
                        title = title.split(' - ')[0]
                    if title and len(title) > 5:
                        news_titles.append(title)
            
            print(f"êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ {len(news_titles)}ê°œì˜ ë‰´ìŠ¤ ì œëª©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return news_titles
            
        except Exception as e:
            print(f"êµ¬ê¸€ ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return []
    
    def crawl_yna_news(self, keyword="ë°˜ë„ì²´"):
        """
        ì—°í•©ë‰´ìŠ¤ì—ì„œ íŠ¹ì • í‚¤ì›Œë“œ ê´€ë ¨ ë‰´ìŠ¤ ì œëª©ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
        
        Args:
            keyword (str): ê²€ìƒ‰í•  í‚¤ì›Œë“œ
            
        Returns:
            list: ë‰´ìŠ¤ ì œëª© ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ì—°í•©ë‰´ìŠ¤ ê²€ìƒ‰ URL
            url = f"https://www.yna.co.kr/search/index?query={keyword}"
            print(f"ì—°í•©ë‰´ìŠ¤ ê²€ìƒ‰: {url}")
            
            response = self.session.get(url)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            news_titles = []
            
            # ì—°í•©ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ
            title_elements = soup.find_all('strong', class_='tit-news')
            
            for element in title_elements:
                link = element.find('a')
                if link:
                    title = link.get_text().strip()
                    if title and len(title) > 5:
                        news_titles.append(title)
            
            print(f"ì—°í•©ë‰´ìŠ¤ì—ì„œ {len(news_titles)}ê°œì˜ ë‰´ìŠ¤ ì œëª©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return news_titles
            
        except Exception as e:
            print(f"ì—°í•©ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return []
    
    def crawl_sample_news_data(self):
        """
        ìƒ˜í”Œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (í¬ë¡¤ë§ì´ ëª¨ë‘ ì‹¤íŒ¨í•  ê²½ìš° ë°ëª¨ìš©)
        
        Returns:
            list: ìƒ˜í”Œ ë‰´ìŠ¤ ì œëª© ë¦¬ìŠ¤íŠ¸
        """
        sample_titles = [
            "ì‚¼ì„±ì „ì, 3ë¶„ê¸° ë°˜ë„ì²´ ì‹¤ì  ê°œì„  ì „ë§",
            "SKí•˜ì´ë‹‰ìŠ¤, HBM ë©”ëª¨ë¦¬ ê³µê¸‰ í™•ëŒ€",
            "ë¯¸êµ­ ë°˜ë„ì²´ ì§€ì›ë²•ìœ¼ë¡œ êµ­ë‚´ ê¸°ì—… ìˆ˜í˜œ",
            "ë°˜ë„ì²´ ê´€ë ¨ì£¼ ìƒìŠ¹ì„¸ ì§€ì†",
            "ë©”ëª¨ë¦¬ ë°˜ë„ì²´ ê°€ê²© ìƒìŠ¹ ê¸°ëŒ€ê°",
            "TSMC, 3ë‚˜ë…¸ ê³µì • ì–‘ì‚° ë³¸ê²©í™”",
            "ë°˜ë„ì²´ ì¥ë¹„ì£¼ ê¸‰ë“±, íˆ¬ìì ê´€ì‹¬ ì§‘ì¤‘",
            "ì¤‘êµ­ ë°˜ë„ì²´ ê·œì œë¡œ êµ­ë‚´ ê¸°ì—… ê¸°íšŒ",
            "AI ë°˜ë„ì²´ ìˆ˜ìš” ê¸‰ì¦ìœ¼ë¡œ ê´€ë ¨ì£¼ ì£¼ëª©",
            "ë°˜ë„ì²´ ì‚¬ì´í´ íšŒë³µ ì‹ í˜¸ í¬ì°©"
        ]
        
        print(f"ìƒ˜í”Œ ë°ì´í„° ìƒì„±: {len(sample_titles)}ê°œì˜ ë‰´ìŠ¤ ì œëª©")
        return sample_titles
    
    def save_to_csv(self, titles, filename='alternative_news_titles.csv'):
        """
        í¬ë¡¤ë§í•œ ì œëª©ë“¤ì„ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            titles (list): ë‰´ìŠ¤ ì œëª© ë¦¬ìŠ¤íŠ¸
            filename (str): ì €ì¥í•  íŒŒì¼ëª…
        """
        try:
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow(['ë²ˆí˜¸', 'ë‰´ìŠ¤ ì œëª©', 'ìˆ˜ì§‘ì‹œê°„'])  # í—¤ë”
                
                current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                for idx, title in enumerate(titles, 1):
                    writer.writerow([idx, title, current_time])
                    
            print(f"ê²°ê³¼ê°€ {filename} íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def save_to_excel(self, titles, filename='result.xlsx'):
        """
        í¬ë¡¤ë§í•œ ì œëª©ë“¤ì„ Excel íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤ (openpyxl ì‚¬ìš©).
        
        Args:
            titles (list): ë‰´ìŠ¤ ì œëª© ë¦¬ìŠ¤íŠ¸
            filename (str): ì €ì¥í•  íŒŒì¼ëª…
        """
        try:
            # ìƒˆ ì›Œí¬ë¶ ìƒì„±
            wb = Workbook()
            ws = wb.active
            ws.title = "ë‰´ìŠ¤ ì œëª©"
            
            # í—¤ë” ìŠ¤íƒ€ì¼ ì„¤ì •
            header_font = Font(bold=True, color="FFFFFF")
            header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
            header_border = Border(
                left=Side(style='thin'),
                right=Side(style='thin'),
                top=Side(style='thin'),
                bottom=Side(style='thin')
            )
            center_alignment = Alignment(horizontal='center', vertical='center')
            
            # í—¤ë” ì‘ì„±
            headers = ['ë²ˆí˜¸', 'ë‰´ìŠ¤ ì œëª©', 'ìˆ˜ì§‘ì‹œê°„']
            for col, header in enumerate(headers, 1):
                cell = ws.cell(row=1, column=col, value=header)
                cell.font = header_font
                cell.fill = header_fill
                cell.border = header_border
                cell.alignment = center_alignment
            
            # ë°ì´í„° ì…ë ¥
            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            for idx, title in enumerate(titles, 1):
                # ë²ˆí˜¸
                ws.cell(row=idx+1, column=1, value=idx).alignment = center_alignment
                # ì œëª©
                ws.cell(row=idx+1, column=2, value=title)
                # ì‹œê°„
                ws.cell(row=idx+1, column=3, value=current_time).alignment = center_alignment
            
            # ì»¬ëŸ¼ ë„ˆë¹„ ìë™ ì¡°ì •
            ws.column_dimensions['A'].width = 8   # ë²ˆí˜¸
            ws.column_dimensions['B'].width = 80  # ì œëª© (ë„“ê²Œ)
            ws.column_dimensions['C'].width = 20  # ì‹œê°„
            
            # ë°ì´í„° ì˜ì—­ í…Œë‘ë¦¬ ì¶”ê°€
            data_border = Border(
                left=Side(style='thin'),
                right=Side(style='thin'),
                top=Side(style='thin'),
                bottom=Side(style='thin')
            )
            
            for row in range(2, len(titles) + 2):
                for col in range(1, 4):
                    ws.cell(row=row, column=col).border = data_border
            
            # ì œëª© í–‰ ê³ ì •
            ws.freeze_panes = 'A2'
            
            # íŒŒì¼ ì €ì¥
            wb.save(filename)
            print(f"ğŸ“Š Excel íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}")
            print(f"   - ì´ {len(titles)}ê°œì˜ ë‰´ìŠ¤ ì œëª©")
            print(f"   - ìˆ˜ì§‘ ì‹œê°„: {current_time}")
            
        except Exception as e:
            print(f"Excel íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def print_titles(self, titles, source=""):
        """
        í¬ë¡¤ë§í•œ ì œëª©ë“¤ì„ ì½˜ì†”ì— ì¶œë ¥í•©ë‹ˆë‹¤.
        
        Args:
            titles (list): ë‰´ìŠ¤ ì œëª© ë¦¬ìŠ¤íŠ¸
            source (str): ì¶œì²˜ ì •ë³´
        """
        print("\n" + "="*60)
        if source:
            print(f"[{source}] í¬ë¡¤ë§ëœ ë‰´ìŠ¤ ì œëª©ë“¤:")
        else:
            print("í¬ë¡¤ë§ëœ ë‰´ìŠ¤ ì œëª©ë“¤:")
        print("="*60)
        
        for idx, title in enumerate(titles, 1):
            print(f"{idx:2d}. {title}")
        
        print("="*60)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    crawler = AlternativeNewsCrawler()
    
    print("ëŒ€ì•ˆì ì¸ ë‰´ìŠ¤ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("í‚¤ì›Œë“œ: ë°˜ë„ì²´ ê´€ë ¨ì£¼\n")
    
    all_titles = []
    
    # 1. ë‹¤ìŒ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œë„
    print("1. ë‹¤ìŒ ë‰´ìŠ¤ì—ì„œ í¬ë¡¤ë§ ì‹œë„...")
    daum_titles = crawler.crawl_daum_news("ë°˜ë„ì²´")
    if daum_titles:
        crawler.print_titles(daum_titles, "ë‹¤ìŒ ë‰´ìŠ¤")
        all_titles.extend(daum_titles)
    
    time.sleep(2)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
    
    # 2. êµ¬ê¸€ ë‰´ìŠ¤ RSS í¬ë¡¤ë§ ì‹œë„
    print("\n2. êµ¬ê¸€ ë‰´ìŠ¤ RSSì—ì„œ í¬ë¡¤ë§ ì‹œë„...")
    google_titles = crawler.crawl_google_news("ë°˜ë„ì²´ ê´€ë ¨ì£¼")
    if google_titles:
        crawler.print_titles(google_titles, "êµ¬ê¸€ ë‰´ìŠ¤")
        # ì¤‘ë³µ ì œê±°í•˜ì—¬ ì¶”ê°€
        for title in google_titles:
            if title not in all_titles:
                all_titles.append(title)
    
    time.sleep(2)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
    
    # 3. ì—°í•©ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œë„
    print("\n3. ì—°í•©ë‰´ìŠ¤ì—ì„œ í¬ë¡¤ë§ ì‹œë„...")
    yna_titles = crawler.crawl_yna_news("ë°˜ë„ì²´")
    if yna_titles:
        crawler.print_titles(yna_titles, "ì—°í•©ë‰´ìŠ¤")
        # ì¤‘ë³µ ì œê±°í•˜ì—¬ ì¶”ê°€
        for title in yna_titles:
            if title not in all_titles:
                all_titles.append(title)
    
    # 4. ëª¨ë“  í¬ë¡¤ë§ì´ ì‹¤íŒ¨í•œ ê²½ìš° ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©
    if not all_titles:
        print("\n4. ëª¨ë“  í¬ë¡¤ë§ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
        sample_titles = crawler.crawl_sample_news_data()
        all_titles.extend(sample_titles)
        crawler.print_titles(sample_titles, "ìƒ˜í”Œ ë°ì´í„°")
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
    if all_titles:
        print(f"\nğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½:")
        print(f"- ë‹¤ìŒ ë‰´ìŠ¤: {len(daum_titles)}ê°œ")
        print(f"- êµ¬ê¸€ ë‰´ìŠ¤: {len(google_titles)}ê°œ") 
        print(f"- ì—°í•©ë‰´ìŠ¤: {len(yna_titles)}ê°œ")
        print(f"- ì´ ìˆ˜ì§‘: {len(all_titles)}ê°œ (ì¤‘ë³µ ì œê±°)")
        
        # ì „ì²´ ê²°ê³¼ ì¶œë ¥
        crawler.print_titles(all_titles, "ì „ì²´ ìˆ˜ì§‘ ê²°ê³¼")
        
        # íŒŒì¼ ì €ì¥
        crawler.save_to_csv(all_titles, 'alternative_semiconductor_news.csv')
        crawler.save_to_excel(all_titles, 'result.xlsx')
        
        print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(all_titles)}ê°œì˜ ë‰´ìŠ¤ ì œëª©ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        print("ğŸ“ ì €ì¥ëœ íŒŒì¼:")
        print("   - alternative_semiconductor_news.csv (CSV í˜•ì‹)")
        print("   - result.xlsx (Excel í˜•ì‹)")
    else:
        print("âŒ ë‰´ìŠ¤ ì œëª©ì„ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
